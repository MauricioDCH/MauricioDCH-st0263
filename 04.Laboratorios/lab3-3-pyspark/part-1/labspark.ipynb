{
  "metadata": {
    "name": "labspark",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# WORDCOUNT COMPACTO\n# Cargar desde HDFS\nfiles_rdd \u003d sc.textFile(\"hdfs:///user/hadoop/datasets/gutenberg-small/*.txt\")\n\n# O cargar desde S3\n#files_rdd \u003d sc.textFile(\"s3://datasets-mauricio/datasets/gutenberg-small/*.txt\")\n\n# Separar líneas en palabras y contar ocurrencias\nwc_unsort \u003d files_rdd.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n\n# Ordenar palabras por frecuencia (descendente)\nwc \u003d wc_unsort.sortBy(lambda a: -a[1])\n\n# Mostrar las 10 palabras más frecuentes\nfor tupla in wc.take(10):\n    print(tupla)\n\n# Guardar el resultado como múltiples archivos RDD\nwc.saveAsTextFile(\"hdfs:///tmp/wcout1\")\n\n# Consolidar el resultado en un solo archivo.\nwc.coalesce(1).saveAsTextFile(\"hdfs:///user/hadoop/wcout4\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}